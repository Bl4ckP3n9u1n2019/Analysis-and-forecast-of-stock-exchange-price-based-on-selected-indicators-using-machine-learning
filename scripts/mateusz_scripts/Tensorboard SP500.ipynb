{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow to open-source’owy framework stworzony przez Google’a do obliczeń numerycznych. \n",
    "# Oferuje on zestaw narzędzi służących do projektowania, trenowania oraz douczania (ang. fine-tuning) sieci neuronowych.\n",
    "# TensorBoard to narzędzie służące do wizualizacji danych. Pozwala prezentować w czytelny sposób m.in. grafy obliczeniowe \n",
    "# sieci neuronowych (ang. computational graph), dane wejściowe, dane wyjściowe czy postęp treningu. Dzięki temu ułatwia \n",
    "# ono znajdowanie błędów w architekturze oraz optymalizację sieci neuronowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software requirements\n",
    "# NVIDIA® GPU drivers —CUDA 10.1 requires 418.x or higher.\n",
    "# CUDA® Toolkit —TensorFlow supports CUDA 10.1 (TensorFlow >= 2.1.0)\n",
    "# CUPTI ships with the CUDA Toolkit.\n",
    "# cuDNN SDK (>= 7.6)\n",
    "#(Optional) TensorRT 6.0 to improve latency and throughput for inference on some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Python for Data Science and Machine Learning Bootcamp, kurs z platformy Udemy, Stworzony przez Jose Portilla\n",
    "# Ostatnia aktualizacja: 5/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Mateusz\\Desktop\\WSBProjekt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data\\predictions.db')\n",
    "c = conn.cursor()\n",
    "indicator=\"SP500\"\n",
    "# SELECT QUERY dla indexu SP500\n",
    "df = pd.read_sql_query(\"\"\"select x.\"Date\", co.\"Index\" as Crude_Oil, \n",
    "cr.\"Index\" as Copper, s.\"Index\" as Silver, p.\"Index\" as Platinum,\n",
    "ip.\"Index\" as Industrial_Production, pa.\"Index\" as Palladium, x.\"Index\" as \"\"\"+indicator+\"\"\"\n",
    " from \"\"\"+indicator+\"\"\" as x\n",
    "  LEFT OUTER JOIN Crude_Oil as co ON x.Date=co.Date\n",
    "  LEFT OUTER JOIN Copper as cr ON x.Date=cr.Date\n",
    "  LEFT OUTER JOIN Silver as s ON x.Date=s.Date\n",
    "  LEFT OUTER JOIN Platinum as p ON x.Date=p.Date\n",
    "  LEFT OUTER JOIN Industrial_Prod as ip ON strftime('%Y-%m', x.Date)=strftime('%Y-%m', ip.Date)\n",
    "  LEFT OUTER JOIN Palladium as pa ON x.Date=pa.Date\n",
    "Where x.\"Index\" and co.\"Index\" and cr.\"Index\" and s.\"Index\" and p.\"Index\"\n",
    "and ip.\"Index\" and pa.\"Index\" IS NOT NULL\n",
    "and x.Date < '2020-04-01'\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Crude_Oil</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Silver</th>\n",
       "      <th>Platinum</th>\n",
       "      <th>Industrial_Production</th>\n",
       "      <th>Palladium</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04 00:00:00</td>\n",
       "      <td>81.52</td>\n",
       "      <td>3.4060</td>\n",
       "      <td>17.170</td>\n",
       "      <td>1523.90</td>\n",
       "      <td>91.685</td>\n",
       "      <td>421.40</td>\n",
       "      <td>1132.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-06 00:00:00</td>\n",
       "      <td>83.12</td>\n",
       "      <td>3.4945</td>\n",
       "      <td>17.890</td>\n",
       "      <td>1558.40</td>\n",
       "      <td>91.685</td>\n",
       "      <td>427.20</td>\n",
       "      <td>1137.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-07 00:00:00</td>\n",
       "      <td>82.60</td>\n",
       "      <td>3.4270</td>\n",
       "      <td>18.090</td>\n",
       "      <td>1559.40</td>\n",
       "      <td>91.685</td>\n",
       "      <td>424.55</td>\n",
       "      <td>1141.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-08 00:00:00</td>\n",
       "      <td>82.74</td>\n",
       "      <td>3.4005</td>\n",
       "      <td>18.120</td>\n",
       "      <td>1570.60</td>\n",
       "      <td>91.685</td>\n",
       "      <td>425.15</td>\n",
       "      <td>1144.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-11 00:00:00</td>\n",
       "      <td>82.54</td>\n",
       "      <td>3.4410</td>\n",
       "      <td>18.840</td>\n",
       "      <td>1592.50</td>\n",
       "      <td>91.685</td>\n",
       "      <td>431.95</td>\n",
       "      <td>1146.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>2020-03-25 00:00:00</td>\n",
       "      <td>24.49</td>\n",
       "      <td>2.2040</td>\n",
       "      <td>13.965</td>\n",
       "      <td>745.50</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2247.60</td>\n",
       "      <td>2475.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>2020-03-26 00:00:00</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.1780</td>\n",
       "      <td>14.415</td>\n",
       "      <td>737.10</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2226.10</td>\n",
       "      <td>2630.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2020-03-27 00:00:00</td>\n",
       "      <td>21.51</td>\n",
       "      <td>2.1720</td>\n",
       "      <td>14.315</td>\n",
       "      <td>740.82</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2196.80</td>\n",
       "      <td>2541.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2020-03-30 00:00:00</td>\n",
       "      <td>20.09</td>\n",
       "      <td>2.1555</td>\n",
       "      <td>14.055</td>\n",
       "      <td>723.84</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2197.60</td>\n",
       "      <td>2626.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "      <td>20.48</td>\n",
       "      <td>2.2280</td>\n",
       "      <td>13.930</td>\n",
       "      <td>728.80</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2304.80</td>\n",
       "      <td>2584.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2515 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date  Crude_Oil  Copper  Silver  Platinum  \\\n",
       "0     2010-01-04 00:00:00      81.52  3.4060  17.170   1523.90   \n",
       "1     2010-01-06 00:00:00      83.12  3.4945  17.890   1558.40   \n",
       "2     2010-01-07 00:00:00      82.60  3.4270  18.090   1559.40   \n",
       "3     2010-01-08 00:00:00      82.74  3.4005  18.120   1570.60   \n",
       "4     2010-01-11 00:00:00      82.54  3.4410  18.840   1592.50   \n",
       "...                   ...        ...     ...     ...       ...   \n",
       "2510  2020-03-25 00:00:00      24.49  2.2040  13.965    745.50   \n",
       "2511  2020-03-26 00:00:00      22.60  2.1780  14.415    737.10   \n",
       "2512  2020-03-27 00:00:00      21.51  2.1720  14.315    740.82   \n",
       "2513  2020-03-30 00:00:00      20.09  2.1555  14.055    723.84   \n",
       "2514  2020-03-31 00:00:00      20.48  2.2280  13.930    728.80   \n",
       "\n",
       "      Industrial_Production  Palladium    SP500  \n",
       "0                    91.685     421.40  1132.99  \n",
       "1                    91.685     427.20  1137.14  \n",
       "2                    91.685     424.55  1141.69  \n",
       "3                    91.685     425.15  1144.98  \n",
       "4                    91.685     431.95  1146.98  \n",
       "...                     ...        ...      ...  \n",
       "2510                103.664    2247.60  2475.56  \n",
       "2511                103.664    2226.10  2630.07  \n",
       "2512                103.664    2196.80  2541.47  \n",
       "2513                103.664    2197.60  2626.65  \n",
       "2514                103.664    2304.80  2584.59  \n",
       "\n",
       "[2515 rows x 8 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ze względu na to że dane trainowe mają mieć wartości float, usuwam kolumne daty\n",
    "df = df.drop('Date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crude_Oil</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Silver</th>\n",
       "      <th>Platinum</th>\n",
       "      <th>Industrial_Production</th>\n",
       "      <th>Palladium</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.52</td>\n",
       "      <td>3.4060</td>\n",
       "      <td>17.170</td>\n",
       "      <td>1523.90</td>\n",
       "      <td>91.685</td>\n",
       "      <td>421.40</td>\n",
       "      <td>1132.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.12</td>\n",
       "      <td>3.4945</td>\n",
       "      <td>17.890</td>\n",
       "      <td>1558.40</td>\n",
       "      <td>91.685</td>\n",
       "      <td>427.20</td>\n",
       "      <td>1137.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.60</td>\n",
       "      <td>3.4270</td>\n",
       "      <td>18.090</td>\n",
       "      <td>1559.40</td>\n",
       "      <td>91.685</td>\n",
       "      <td>424.55</td>\n",
       "      <td>1141.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82.74</td>\n",
       "      <td>3.4005</td>\n",
       "      <td>18.120</td>\n",
       "      <td>1570.60</td>\n",
       "      <td>91.685</td>\n",
       "      <td>425.15</td>\n",
       "      <td>1144.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.54</td>\n",
       "      <td>3.4410</td>\n",
       "      <td>18.840</td>\n",
       "      <td>1592.50</td>\n",
       "      <td>91.685</td>\n",
       "      <td>431.95</td>\n",
       "      <td>1146.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>24.49</td>\n",
       "      <td>2.2040</td>\n",
       "      <td>13.965</td>\n",
       "      <td>745.50</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2247.60</td>\n",
       "      <td>2475.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>22.60</td>\n",
       "      <td>2.1780</td>\n",
       "      <td>14.415</td>\n",
       "      <td>737.10</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2226.10</td>\n",
       "      <td>2630.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>21.51</td>\n",
       "      <td>2.1720</td>\n",
       "      <td>14.315</td>\n",
       "      <td>740.82</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2196.80</td>\n",
       "      <td>2541.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>20.09</td>\n",
       "      <td>2.1555</td>\n",
       "      <td>14.055</td>\n",
       "      <td>723.84</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2197.60</td>\n",
       "      <td>2626.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>20.48</td>\n",
       "      <td>2.2280</td>\n",
       "      <td>13.930</td>\n",
       "      <td>728.80</td>\n",
       "      <td>103.664</td>\n",
       "      <td>2304.80</td>\n",
       "      <td>2584.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2515 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Crude_Oil  Copper  Silver  Platinum  Industrial_Production  Palladium  \\\n",
       "0         81.52  3.4060  17.170   1523.90                 91.685     421.40   \n",
       "1         83.12  3.4945  17.890   1558.40                 91.685     427.20   \n",
       "2         82.60  3.4270  18.090   1559.40                 91.685     424.55   \n",
       "3         82.74  3.4005  18.120   1570.60                 91.685     425.15   \n",
       "4         82.54  3.4410  18.840   1592.50                 91.685     431.95   \n",
       "...         ...     ...     ...       ...                    ...        ...   \n",
       "2510      24.49  2.2040  13.965    745.50                103.664    2247.60   \n",
       "2511      22.60  2.1780  14.415    737.10                103.664    2226.10   \n",
       "2512      21.51  2.1720  14.315    740.82                103.664    2196.80   \n",
       "2513      20.09  2.1555  14.055    723.84                103.664    2197.60   \n",
       "2514      20.48  2.2280  13.930    728.80                103.664    2304.80   \n",
       "\n",
       "        SP500  \n",
       "0     1132.99  \n",
       "1     1137.14  \n",
       "2     1141.69  \n",
       "3     1144.98  \n",
       "4     1146.98  \n",
       "...       ...  \n",
       "2510  2475.56  \n",
       "2511  2630.07  \n",
       "2512  2541.47  \n",
       "2513  2626.65  \n",
       "2514  2584.59  \n",
       "\n",
       "[2515 rows x 7 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training And Test Data\n",
    "X = df.drop('SP500',axis=1).values\n",
    "y = df['SP500'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1132.99, 1137.14, 1141.69, ..., 2541.47, 2626.65, 2584.59])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.15200e+01, 3.40600e+00, 1.71700e+01, 1.52390e+03, 9.16850e+01,\n",
       "        4.21400e+02],\n",
       "       [8.31200e+01, 3.49450e+00, 1.78900e+01, 1.55840e+03, 9.16850e+01,\n",
       "        4.27200e+02],\n",
       "       [8.26000e+01, 3.42700e+00, 1.80900e+01, 1.55940e+03, 9.16850e+01,\n",
       "        4.24550e+02],\n",
       "       ...,\n",
       "       [2.15100e+01, 2.17200e+00, 1.43150e+01, 7.40820e+02, 1.03664e+02,\n",
       "        2.19680e+03],\n",
       "       [2.00900e+01, 2.15550e+00, 1.40550e+01, 7.23840e+02, 1.03664e+02,\n",
       "        2.19760e+03],\n",
       "       [2.04800e+01, 2.22800e+00, 1.39300e+01, 7.28800e+02, 1.03664e+02,\n",
       "        2.30480e+03]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dzielenie tablic na losowe podzbiory trainowe i testowe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalling data\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-05-06--1810'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%Y-%m-%d--%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS: Use \"logs\\\\fit\"\n",
    "# MACOS/LINUX: Use \"logs\\fit\"\n",
    "\n",
    "log_directory = 'logs\\\\fit'\n",
    "\n",
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teraz utwórzę warstwy modelu\n",
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1886 samples, validate on 629 samples\n",
      "Epoch 1/600\n",
      "1886/1886 [==============================] - 1s 419us/sample - loss: -4708.3915 - val_loss: -8285.0848\n",
      "Epoch 2/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -15904.4233 - val_loss: -26136.2878\n",
      "Epoch 3/600\n",
      "1886/1886 [==============================] - 0s 210us/sample - loss: -44273.8878 - val_loss: -67926.8922\n",
      "Epoch 4/600\n",
      "1886/1886 [==============================] - 0s 143us/sample - loss: -105463.7195 - val_loss: -150515.0403\n",
      "Epoch 5/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -213539.8454 - val_loss: -288960.6156\n",
      "Epoch 6/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -391970.8112 - val_loss: -515607.6834\n",
      "Epoch 7/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -686673.5766 - val_loss: -852242.0722\n",
      "Epoch 8/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -1078661.8034 - val_loss: -1301462.8041\n",
      "Epoch 9/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -1596270.1637 - val_loss: -1876369.3217\n",
      "Epoch 10/600\n",
      "1886/1886 [==============================] - 0s 123us/sample - loss: -2209700.8070 - val_loss: -2582040.9519\n",
      "Epoch 11/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -3025408.3115 - val_loss: -3442028.9956\n",
      "Epoch 12/600\n",
      "1886/1886 [==============================] - 0s 130us/sample - loss: -3991744.7590 - val_loss: -4470731.2583\n",
      "Epoch 13/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -5143590.6707 - val_loss: -5668451.1971\n",
      "Epoch 14/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -6453846.4624 - val_loss: -7057767.1169\n",
      "Epoch 15/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -7720602.2577 - val_loss: -8595813.8283\n",
      "Epoch 16/600\n",
      "1886/1886 [==============================] - 0s 135us/sample - loss: -9549224.8526 - val_loss: -10357133.2401\n",
      "Epoch 17/600\n",
      "1886/1886 [==============================] - 0s 151us/sample - loss: -11480756.4857 - val_loss: -12337857.3752\n",
      "Epoch 18/600\n",
      "1886/1886 [==============================] - 0s 163us/sample - loss: -13521705.1474 - val_loss: -14541352.1510\n",
      "Epoch 19/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -16305953.9024 - val_loss: -17066712.3148\n",
      "Epoch 20/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -18774922.2587 - val_loss: -19846706.5660\n",
      "Epoch 21/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -21736548.7593 - val_loss: -22880753.5199\n",
      "Epoch 22/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -25017065.1177 - val_loss: -26169189.2178\n",
      "Epoch 23/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -28471919.9936 - val_loss: -29758618.0413\n",
      "Epoch 24/600\n",
      "1886/1886 [==============================] - 0s 131us/sample - loss: -31814759.5249 - val_loss: -33641231.6757\n",
      "Epoch 25/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -36747519.5843 - val_loss: -37930577.1383\n",
      "Epoch 26/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -41242328.8229 - val_loss: -42580995.7138\n",
      "Epoch 27/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -45542604.4666 - val_loss: -47522314.6200\n",
      "Epoch 28/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -50490978.8208 - val_loss: -52719059.0652\n",
      "Epoch 29/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -55871088.7041 - val_loss: -58252207.6630\n",
      "Epoch 30/600\n",
      "1886/1886 [==============================] - 0s 225us/sample - loss: -63825632.1273 - val_loss: -64314341.9269\n",
      "Epoch 31/600\n",
      "1886/1886 [==============================] - 0s 222us/sample - loss: -69722334.0997 - val_loss: -70655868.7440\n",
      "Epoch 32/600\n",
      "1886/1886 [==============================] - 0s 200us/sample - loss: -74212942.5069 - val_loss: -77200776.7631\n",
      "Epoch 33/600\n",
      "1886/1886 [==============================] - 0s 121us/sample - loss: -81482150.5069 - val_loss: -84071851.8283\n",
      "Epoch 34/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -89024305.1622 - val_loss: -91367309.1002\n",
      "Epoch 35/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -95225130.4008 - val_loss: -98937706.9253\n",
      "Epoch 36/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -102501239.3637 - val_loss: -106824145.6025\n",
      "Epoch 37/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -111719986.7741 - val_loss: -115066196.7313\n",
      "Epoch 38/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -122222290.6808 - val_loss: -123882562.7091\n",
      "Epoch 39/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -130101447.4995 - val_loss: -133003456.1399\n",
      "Epoch 40/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -140392264.9077 - val_loss: -142497057.0684\n",
      "Epoch 41/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -151958283.8261 - val_loss: -152484206.5755\n",
      "Epoch 42/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -159128477.4889 - val_loss: -162722492.9984\n",
      "Epoch 43/600\n",
      "1886/1886 [==============================] - 0s 197us/sample - loss: -170036838.8717 - val_loss: -173263095.7329\n",
      "Epoch 44/600\n",
      "1886/1886 [==============================] - 0s 182us/sample - loss: -179500718.4390 - val_loss: -184094101.9014\n",
      "Epoch 45/600\n",
      "1886/1886 [==============================] - 0s 171us/sample - loss: -191396808.3139 - val_loss: -195413371.3959\n",
      "Epoch 46/600\n",
      "1886/1886 [==============================] - 0s 135us/sample - loss: -203695558.8717 - val_loss: -207159712.5087\n",
      "Epoch 47/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -213926429.9639 - val_loss: -219151566.8553\n",
      "Epoch 48/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -226061872.3393 - val_loss: -231602555.9555\n",
      "Epoch 49/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -243883411.8006 - val_loss: -244645019.4722\n",
      "Epoch 50/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -256069942.9565 - val_loss: -257948261.8506\n",
      "Epoch 51/600\n",
      "1886/1886 [==============================] - 0s 135us/sample - loss: -266292675.9703 - val_loss: -271665020.8967\n",
      "Epoch 52/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -280219269.5313 - val_loss: -285683957.9269\n",
      "Epoch 53/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -299476205.2344 - val_loss: -300273979.9300\n",
      "Epoch 54/600\n",
      "1886/1886 [==============================] - 0s 129us/sample - loss: -313336814.2863 - val_loss: -315328195.2051\n",
      "Epoch 55/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -325450886.3457 - val_loss: -330657739.4976\n",
      "Epoch 56/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -343695442.8335 - val_loss: -346378382.4992\n",
      "Epoch 57/600\n",
      "1886/1886 [==============================] - 0s 137us/sample - loss: -361475614.9141 - val_loss: -362681659.1161\n",
      "Epoch 58/600\n",
      "1886/1886 [==============================] - 0s 164us/sample - loss: -379907224.0255 - val_loss: -379428763.1161\n",
      "Epoch 59/600\n",
      "1886/1886 [==============================] - 0s 139us/sample - loss: -396926686.4051 - val_loss: -396572185.6916\n",
      "Epoch 60/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -413406746.9438 - val_loss: -414184050.5692\n",
      "Epoch 61/600\n",
      "1886/1886 [==============================] - 0s 133us/sample - loss: -426568619.1644 - val_loss: -431934071.3514\n",
      "Epoch 62/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -444941790.0318 - val_loss: -450044688.3816\n",
      "Epoch 63/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -466942216.0085 - val_loss: -468892689.6534\n",
      "Epoch 64/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -480456366.4221 - val_loss: -487791926.4356\n",
      "Epoch 65/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -507332812.5896 - val_loss: -507536143.4149\n",
      "Epoch 66/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -522674022.0064 - val_loss: -527614822.5628\n",
      "Epoch 67/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -546501988.7169 - val_loss: -547957845.9269\n",
      "Epoch 68/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -569542515.1729 - val_loss: -569117836.5151\n",
      "Epoch 69/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -590840003.6649 - val_loss: -590578315.0906\n",
      "Epoch 70/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -609654048.4411 - val_loss: -612478409.4626\n",
      "Epoch 71/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -633509251.1220 - val_loss: -634685554.5692\n",
      "Epoch 72/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -661195227.2831 - val_loss: -657508622.4483\n",
      "Epoch 73/600\n",
      "1886/1886 [==============================] - 0s 149us/sample - loss: -681214987.3340 - val_loss: -680758068.2989\n",
      "Epoch 74/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -704792895.4571 - val_loss: -704550647.1479\n",
      "Epoch 75/600\n",
      "1886/1886 [==============================] - 0s 186us/sample - loss: -729881573.2598 - val_loss: -728776406.4865\n",
      "Epoch 76/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -763379304.1103 - val_loss: -753462130.1622\n",
      "Epoch 77/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -780101219.0201 - val_loss: -778662585.5898\n",
      "Epoch 78/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -810927551.7285 - val_loss: -804301461.9777\n",
      "Epoch 79/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -842042235.1813 - val_loss: -830601020.6423\n",
      "Epoch 80/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -866634599.2280 - val_loss: -857423940.4769\n",
      "Epoch 81/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -878956928.2036 - val_loss: -884068560.9921\n",
      "Epoch 82/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -914985297.7137 - val_loss: -911327850.4293\n",
      "Epoch 83/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -939738804.3945 - val_loss: -939320278.8935\n",
      "Epoch 84/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -968261454.2524 - val_loss: -967528385.2210\n",
      "Epoch 85/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -991043714.7147 - val_loss: -996372294.8172\n",
      "Epoch 86/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -1020552444.0636 - val_loss: -1025345930.1749\n",
      "Epoch 87/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -1051741661.0477 - val_loss: -1055030550.3847\n",
      "Epoch 88/600\n",
      "1886/1886 [==============================] - 0s 190us/sample - loss: -1101257356.8950 - val_loss: -1085789245.0493\n",
      "Epoch 89/600\n",
      "1886/1886 [==============================] - 1s 314us/sample - loss: -1100234728.2460 - val_loss: -1116128571.6248\n",
      "Epoch 90/600\n",
      "1886/1886 [==============================] - 0s 168us/sample - loss: -1155674617.6204 - val_loss: -1147668690.2130\n",
      "Epoch 91/600\n",
      "1886/1886 [==============================] - 0s 125us/sample - loss: -1176734361.5186 - val_loss: -1179290226.7727\n",
      "Epoch 92/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -1198806799.7455 - val_loss: -1211175707.8792\n",
      "Epoch 93/600\n",
      "1886/1886 [==============================] - 1s 271us/sample - loss: -1239332511.6267 - val_loss: -1243982657.9332\n",
      "Epoch 94/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -1294438569.8070 - val_loss: -1277619199.7965\n",
      "Epoch 95/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -1304911669.0732 - val_loss: -1311378859.9555\n",
      "Epoch 96/600\n",
      "1886/1886 [==============================] - 0s 93us/sample - loss: -1357583069.9300 - val_loss: -1345707321.9968\n",
      "Epoch 97/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -1384311830.9396 - val_loss: -1380746637.6343\n",
      "Epoch 98/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -1419598284.8271 - val_loss: -1416202862.2957\n",
      "Epoch 99/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -1459617726.5748 - val_loss: -1452186314.2766\n",
      "Epoch 100/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -1490677235.3765 - val_loss: -1488633356.6169\n",
      "Epoch 101/600\n",
      "1886/1886 [==============================] - 0s 161us/sample - loss: -1533304432.1188 - val_loss: -1525589986.0859\n",
      "Epoch 102/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -1553804300.4878 - val_loss: -1562882562.0350\n",
      "Epoch 103/600\n",
      "1886/1886 [==============================] - 0s 204us/sample - loss: -1626179205.5652 - val_loss: -1601183179.7011\n",
      "Epoch 104/600\n",
      "1886/1886 [==============================] - 0s 150us/sample - loss: -1654278756.1739 - val_loss: -1639983206.9698\n",
      "Epoch 105/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -1690217097.9088 - val_loss: -1679335499.9046\n",
      "Epoch 106/600\n",
      "1886/1886 [==============================] - 0s 135us/sample - loss: -1736272760.5345 - val_loss: -1719350654.3720\n",
      "Epoch 107/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -1743903234.1718 - val_loss: -1758932745.5644\n",
      "Epoch 108/600\n",
      "1886/1886 [==============================] - 0s 136us/sample - loss: -1808472586.5875 - val_loss: -1799711609.8951\n",
      "Epoch 109/600\n",
      "1886/1886 [==============================] - 0s 143us/sample - loss: -1859110435.0201 - val_loss: -1841269696.1017\n",
      "Epoch 110/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -1882258231.3807 - val_loss: -1883035293.1002\n",
      "Epoch 111/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -1950056324.4793 - val_loss: -1925595754.2258\n",
      "Epoch 112/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -1964787768.1951 - val_loss: -1968112317.2528\n",
      "Epoch 113/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -2009629366.7020 - val_loss: -2011838689.8824\n",
      "Epoch 114/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -2024697864.8229 - val_loss: -2055485050.5056\n",
      "Epoch 115/600\n",
      "1886/1886 [==============================] - 0s 207us/sample - loss: -2114234870.4984 - val_loss: -2099714520.9285\n",
      "Epoch 116/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -2164658754.7826 - val_loss: -2144907314.8744\n",
      "Epoch 117/600\n",
      "1886/1886 [==============================] - 0s 197us/sample - loss: -2197758691.2238 - val_loss: -2190730075.9809\n",
      "Epoch 118/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -2248285109.6161 - val_loss: -2237402416.8394\n",
      "Epoch 119/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -2338738862.0148 - val_loss: -2285179087.1606\n",
      "Epoch 120/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -2332043351.1432 - val_loss: -2332440759.9618\n",
      "Epoch 121/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -2354191356.4708 - val_loss: -2379865932.5151\n",
      "Epoch 122/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -2423098483.6479 - val_loss: -2428081224.4452\n",
      "Epoch 123/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -2460482292.8696 - val_loss: -2477330208.5596\n",
      "Epoch 124/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -2521795363.5631 - val_loss: -2526439201.7806\n",
      "Epoch 125/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -2612304250.9777 - val_loss: -2577271366.4102\n",
      "Epoch 126/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -2629238095.2704 - val_loss: -2628455432.5469\n",
      "Epoch 127/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -2682044059.0117 - val_loss: -2679996745.6661\n",
      "Epoch 128/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -2724927122.8674 - val_loss: -2731798041.6407\n",
      "Epoch 129/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -2838749823.5928 - val_loss: -2785282316.6169\n",
      "Epoch 130/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1886/1886 [==============================] - 0s 172us/sample - loss: -2894180721.2047 - val_loss: -2839538751.4913\n",
      "Epoch 131/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -2867318494.6087 - val_loss: -2893071294.0668\n",
      "Epoch 132/600\n",
      "1886/1886 [==============================] - 0s 146us/sample - loss: -2972110692.7169 - val_loss: -2947697876.0445\n",
      "Epoch 133/600\n",
      "1886/1886 [==============================] - 0s 188us/sample - loss: -2946234884.0721 - val_loss: -3002093408.0509\n",
      "Epoch 134/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -3051746833.1029 - val_loss: -3057399698.1113\n",
      "Epoch 135/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -3112060437.9894 - val_loss: -3113763288.9285\n",
      "Epoch 136/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -3177886209.3574 - val_loss: -3170944061.8633\n",
      "Epoch 137/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -3219033899.7073 - val_loss: -3228616969.7679\n",
      "Epoch 138/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -3337942056.9926 - val_loss: -3287451407.0588\n",
      "Epoch 139/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -3402070366.4730 - val_loss: -3347389334.1812\n",
      "Epoch 140/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -3459214277.3616 - val_loss: -3407368200.1399\n",
      "Epoch 141/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -3457603489.2556 - val_loss: -3467491539.2305\n",
      "Epoch 142/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -3537640877.2004 - val_loss: -3528238425.9459\n",
      "Epoch 143/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -3670545561.1113 - val_loss: -3590827873.2719\n",
      "Epoch 144/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -3681715077.0223 - val_loss: -3653434090.8362\n",
      "Epoch 145/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -3754107638.7699 - val_loss: -3716646777.6916\n",
      "Epoch 146/600\n",
      "1886/1886 [==============================] - 0s 193us/sample - loss: -3810598357.3786 - val_loss: -3780240775.9364\n",
      "Epoch 147/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -3841299628.1145 - val_loss: -3844284169.3609\n",
      "Epoch 148/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -3912669202.1888 - val_loss: -3908265397.5199\n",
      "Epoch 149/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -3983517884.6744 - val_loss: -3973706408.0890\n",
      "Epoch 150/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -4071983222.6341 - val_loss: -4040153831.1733\n",
      "Epoch 151/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -4122869515.1304 - val_loss: -4106701056.4070\n",
      "Epoch 152/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -4207108198.6172 - val_loss: -4174314534.2576\n",
      "Epoch 153/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -4240776936.3818 - val_loss: -4242445964.4134\n",
      "Epoch 154/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -4388969397.6161 - val_loss: -4312023769.3355\n",
      "Epoch 155/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -4474200468.4963 - val_loss: -4382179852.6169\n",
      "Epoch 156/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -4512798826.4178 - val_loss: -4453020447.3386\n",
      "Epoch 157/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -4555211910.1082 - val_loss: -4523584818.0604\n",
      "Epoch 158/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -4666849892.9883 - val_loss: -4596034536.3943\n",
      "Epoch 159/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -4713267204.0721 - val_loss: -4668600801.8824\n",
      "Epoch 160/600\n",
      "1886/1886 [==============================] - 0s 158us/sample - loss: -4810645082.1294 - val_loss: -4741297325.3800\n",
      "Epoch 161/600\n",
      "1886/1886 [==============================] - 0s 183us/sample - loss: -4821255320.0255 - val_loss: -4814979568.5342\n",
      "Epoch 162/600\n",
      "1886/1886 [==============================] - 0s 148us/sample - loss: -4937188607.7285 - val_loss: -4889367089.6534\n",
      "Epoch 163/600\n",
      "1886/1886 [==============================] - 0s 221us/sample - loss: -5033375102.7784 - val_loss: -4964550922.1749\n",
      "Epoch 164/600\n",
      "1886/1886 [==============================] - 0s 119us/sample - loss: -5034852780.3860 - val_loss: -5039846953.5135\n",
      "Epoch 165/600\n",
      "1886/1886 [==============================] - 0s 123us/sample - loss: -5184522916.5133 - val_loss: -5116505551.1606\n",
      "Epoch 166/600\n",
      "1886/1886 [==============================] - 0s 122us/sample - loss: -5316915622.4136 - val_loss: -5194566790.3084\n",
      "Epoch 167/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -5240915507.5801 - val_loss: -5271629683.1797\n",
      "Epoch 168/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -5442452427.3340 - val_loss: -5350246728.8521\n",
      "Epoch 169/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -5480521884.9120 - val_loss: -5429158612.4515\n",
      "Epoch 170/600\n",
      "1886/1886 [==============================] - 0s 140us/sample - loss: -5529198590.3712 - val_loss: -5509372949.1638\n",
      "Epoch 171/600\n",
      "1886/1886 [==============================] - 0s 138us/sample - loss: -5662522441.2980 - val_loss: -5590040647.6312\n",
      "Epoch 172/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -5697627880.3818 - val_loss: -5671274981.9523\n",
      "Epoch 173/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -5700209795.3934 - val_loss: -5752074118.7154\n",
      "Epoch 174/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -5861060859.3849 - val_loss: -5834125036.8712\n",
      "Epoch 175/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -5979133807.0329 - val_loss: -5917959530.2258\n",
      "Epoch 176/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -5948817168.5599 - val_loss: -6001399100.6423\n",
      "Epoch 177/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -6099409825.5270 - val_loss: -6085984215.3005\n",
      "Epoch 178/600\n",
      "1886/1886 [==============================] - 0s 130us/sample - loss: -6331101686.7699 - val_loss: -6172397284.7313\n",
      "Epoch 179/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -6366004363.5376 - val_loss: -6260438843.8283\n",
      "Epoch 180/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -6384752398.3881 - val_loss: -6347704779.9046\n",
      "Epoch 181/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -6456412050.3245 - val_loss: -6435255227.6248\n",
      "Epoch 182/600\n",
      "1886/1886 [==============================] - 0s 134us/sample - loss: -6551176388.0042 - val_loss: -6523684845.2782\n",
      "Epoch 183/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -6731416333.3022 - val_loss: -6613924749.2273\n",
      "Epoch 184/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -6752590878.4051 - val_loss: -6703990639.9237\n",
      "Epoch 185/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -6863538322.0530 - val_loss: -6795449244.6932\n",
      "Epoch 186/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -6887735104.8823 - val_loss: -6886325600.4579\n",
      "Epoch 187/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -7000129960.0424 - val_loss: -6978415311.5676\n",
      "Epoch 188/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -7115132880.2206 - val_loss: -7071680774.9189\n",
      "Epoch 189/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -7215294947.7667 - val_loss: -7165259934.7281\n",
      "Epoch 190/600\n",
      "1886/1886 [==============================] - 0s 199us/sample - loss: -7294425914.3669 - val_loss: -7259917234.6709\n",
      "Epoch 191/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -7466685691.3849 - val_loss: -7356383485.9650\n",
      "Epoch 192/600\n",
      "1886/1886 [==============================] - 0s 209us/sample - loss: -7419989655.4825 - val_loss: -7451843163.9809\n",
      "Epoch 193/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -7582344413.5228 - val_loss: -7548207958.6900\n",
      "Epoch 194/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -7625983734.4984 - val_loss: -7645100850.8744\n",
      "Epoch 195/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -7798477032.3818 - val_loss: -7744027656.1399\n",
      "Epoch 196/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -7952777771.9788 - val_loss: -7844155753.4118\n",
      "Epoch 197/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -7846374474.9268 - val_loss: -7943265136.7377\n",
      "Epoch 198/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -7917699020.9629 - val_loss: -8042171816.9030\n",
      "Epoch 199/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -8145016306.9692 - val_loss: -8143246547.6375\n",
      "Epoch 200/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -8494944292.3775 - val_loss: -8248123718.4102\n",
      "Epoch 201/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -8377072976.6278 - val_loss: -8351578851.9173\n",
      "Epoch 202/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -8347428144.0509 - val_loss: -8454414950.5628\n",
      "Epoch 203/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -8521919436.4199 - val_loss: -8558034681.8951\n",
      "Epoch 204/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -8831724266.0106 - val_loss: -8665620418.1367\n",
      "Epoch 205/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -8869431749.9046 - val_loss: -8772748454.0541\n",
      "Epoch 206/600\n",
      "1886/1886 [==============================] - 0s 152us/sample - loss: -8946013832.8229 - val_loss: -8880079114.9889\n",
      "Epoch 207/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -9068242849.5270 - val_loss: -8989318012.1335\n",
      "Epoch 208/600\n",
      "1886/1886 [==============================] - 0s 166us/sample - loss: -9177917802.6893 - val_loss: -9098097690.0477\n",
      "Epoch 209/600\n",
      "1886/1886 [==============================] - 0s 121us/sample - loss: -9217090042.0276 - val_loss: -9207390079.3895\n",
      "Epoch 210/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -9408806537.9088 - val_loss: -9318896789.7742\n",
      "Epoch 211/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -9649168337.3065 - val_loss: -9432099496.4960\n",
      "Epoch 212/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -9547157883.5207 - val_loss: -9544217093.6979\n",
      "Epoch 213/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -9681310331.2492 - val_loss: -9656803134.2703\n",
      "Epoch 214/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -9870658868.3945 - val_loss: -9770568407.7075\n",
      "Epoch 215/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -9991223110.3118 - val_loss: -9886290974.9316\n",
      "Epoch 216/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -10192476541.1495 - val_loss: -10002873663.0843\n",
      "Epoch 217/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -10169084259.0880 - val_loss: -10120092577.5771\n",
      "Epoch 218/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -10427813717.5143 - val_loss: -10238597933.9905\n",
      "Epoch 219/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -10370304145.5101 - val_loss: -10355816189.1510\n",
      "Epoch 220/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -10451830007.5843 - val_loss: -10474121150.8808\n",
      "Epoch 221/600\n",
      "1886/1886 [==============================] - 0s 124us/sample - loss: -10553496369.6797 - val_loss: -10592002829.4308\n",
      "Epoch 222/600\n",
      "1886/1886 [==============================] - 0s 168us/sample - loss: -10685307365.3955 - val_loss: -10711658989.2782\n",
      "Epoch 223/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -11022655422.8462 - val_loss: -10834880840.8521\n",
      "Epoch 224/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -10826465212.6744 - val_loss: -10955885195.1924\n",
      "Epoch 225/600\n",
      "1886/1886 [==============================] - 0s 129us/sample - loss: -11146973574.9226 - val_loss: -11078086555.0652\n",
      "Epoch 226/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -11265846539.1304 - val_loss: -11202796631.9110\n",
      "Epoch 227/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -11507473213.6246 - val_loss: -11329420751.9746\n",
      "Epoch 228/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -11596085290.3499 - val_loss: -11455676017.1447\n",
      "Epoch 229/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -11526425619.5461 - val_loss: -11581595960.5723\n",
      "Epoch 230/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -11904217719.9915 - val_loss: -11709995498.0223\n",
      "Epoch 231/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -11986777265.0011 - val_loss: -11839942913.2210\n",
      "Epoch 232/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -12050217454.0827 - val_loss: -11968940701.1002\n",
      "Epoch 233/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -12358465692.3690 - val_loss: -12100908899.7138\n",
      "Epoch 234/600\n",
      "1886/1886 [==============================] - 0s 141us/sample - loss: -12579534539.6055 - val_loss: -12234744174.2957\n",
      "Epoch 235/600\n",
      "1886/1886 [==============================] - 0s 123us/sample - loss: -12419440271.8812 - val_loss: -12365948612.1717\n",
      "Epoch 236/600\n",
      "1886/1886 [==============================] - 0s 163us/sample - loss: -12850634721.5949 - val_loss: -12501925329.6025\n",
      "Epoch 237/600\n",
      "1886/1886 [==============================] - 0s 172us/sample - loss: -12871645081.9258 - val_loss: -12637492440.5215\n",
      "Epoch 238/600\n",
      "1886/1886 [==============================] - ETA: 0s - loss: -12668376274.05 - 0s 127us/sample - loss: -12818586018.0700 - val_loss: -12771180221.6598\n",
      "Epoch 239/600\n",
      "1886/1886 [==============================] - 0s 163us/sample - loss: -13008331789.0308 - val_loss: -12907205970.6200\n",
      "Epoch 240/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -13203532207.1007 - val_loss: -13043843039.4404\n",
      "Epoch 241/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -13424446266.3669 - val_loss: -13182082432.2035\n",
      "Epoch 242/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -13467217798.3796 - val_loss: -13321027771.2178\n",
      "Epoch 243/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -13610622586.1633 - val_loss: -13460347501.8887\n",
      "Epoch 244/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -13949934016.4751 - val_loss: -13601852378.5564\n",
      "Epoch 245/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -13930876448.0339 - val_loss: -13744448368.7377\n",
      "Epoch 246/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -14169922292.8696 - val_loss: -13886711762.4165\n",
      "Epoch 247/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -14302276989.1495 - val_loss: -14031045145.2337\n",
      "Epoch 248/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -14456693807.7794 - val_loss: -14176297034.8871\n",
      "Epoch 249/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -14408796585.6713 - val_loss: -14320288463.5676\n",
      "Epoch 250/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -14592098991.3722 - val_loss: -14465386854.1558\n",
      "Epoch 251/600\n",
      "1886/1886 [==============================] - 0s 143us/sample - loss: -14628202385.2386 - val_loss: -14611320755.4849\n",
      "Epoch 252/600\n",
      "1886/1886 [==============================] - 0s 218us/sample - loss: -15088235454.8462 - val_loss: -14760594267.5739\n",
      "Epoch 253/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -14732638548.9714 - val_loss: -14906341156.2226\n",
      "Epoch 254/600\n",
      "1886/1886 [==============================] - 0s 161us/sample - loss: -15240543858.5620 - val_loss: -15055185397.4181\n",
      "Epoch 255/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -15187531991.0074 - val_loss: -15203675590.2067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -15282046266.9099 - val_loss: -15353734279.1224\n",
      "Epoch 257/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -15615050833.4422 - val_loss: -15506051825.7552\n",
      "Epoch 258/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -15976123916.4878 - val_loss: -15660935314.5183\n",
      "Epoch 259/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -15771971352.7041 - val_loss: -15813724654.9062\n",
      "Epoch 260/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -16070474812.8102 - val_loss: -15968070517.6216\n",
      "Epoch 261/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -16297567689.1622 - val_loss: -16124722534.1558\n",
      "Epoch 262/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -16533577523.8515 - val_loss: -16282587469.7361\n",
      "Epoch 263/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -16619520553.8070 - val_loss: -16440528565.5199\n",
      "Epoch 264/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -16786206724.3436 - val_loss: -16600053449.0556\n",
      "Epoch 265/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -16805501580.6235 - val_loss: -16760206801.6025\n",
      "Epoch 266/600\n",
      "1886/1886 [==============================] - 0s 125us/sample - loss: -17226640632.6702 - val_loss: -16921509064.2417\n",
      "Epoch 267/600\n",
      "1886/1886 [==============================] - 0s 143us/sample - loss: -17477618519.6861 - val_loss: -17084962889.2591\n",
      "Epoch 268/600\n",
      "1886/1886 [==============================] - 0s 125us/sample - loss: -17422340893.0477 - val_loss: -17248737249.0684\n",
      "Epoch 269/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -17463158153.0944 - val_loss: -17411671150.7027\n",
      "Epoch 270/600\n",
      "1886/1886 [==============================] - 0s 221us/sample - loss: -17926510492.0976 - val_loss: -17578334050.0858\n",
      "Epoch 271/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -17749014840.7381 - val_loss: -17741936669.3037\n",
      "Epoch 272/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -18338057786.0954 - val_loss: -17912146483.2814\n",
      "Epoch 273/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -17966839956.7678 - val_loss: -18077523583.7965\n",
      "Epoch 274/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -18159595960.8738 - val_loss: -18243919162.2003\n",
      "Epoch 275/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -18858149103.9830 - val_loss: -18415838701.2782\n",
      "Epoch 276/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -18527265135.0329 - val_loss: -18585356835.0016\n",
      "Epoch 277/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -18820836008.8568 - val_loss: -18756171876.9348\n",
      "Epoch 278/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -19351928562.6978 - val_loss: -18930875575.9618\n",
      "Epoch 279/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -19187815679.1856 - val_loss: -19104402922.0223\n",
      "Epoch 280/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -19491722034.7656 - val_loss: -19279892493.0238\n",
      "Epoch 281/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -19956463212.0467 - val_loss: -19458352611.5103\n",
      "Epoch 282/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -19799312349.2513 - val_loss: -19634956519.1733\n",
      "Epoch 283/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -19972266320.6278 - val_loss: -19812951110.0032\n",
      "Epoch 284/600\n",
      "1886/1886 [==============================] - 0s 138us/sample - loss: -20321307074.6469 - val_loss: -19993517855.3386\n",
      "Epoch 285/600\n",
      "1886/1886 [==============================] - 0s 160us/sample - loss: -20912943822.8632 - val_loss: -20176493432.8776\n",
      "Epoch 286/600\n",
      "1886/1886 [==============================] - 0s 150us/sample - loss: -20500773532.9120 - val_loss: -20356276546.3402\n",
      "Epoch 287/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -21002737240.5005 - val_loss: -20541020130.6963\n",
      "Epoch 288/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -20918545752.2291 - val_loss: -20724043681.5771\n",
      "Epoch 289/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -21163084481.8325 - val_loss: -20907442335.5421\n",
      "Epoch 290/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -21409279678.5748 - val_loss: -21093284149.3164\n",
      "Epoch 291/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -21553924901.7349 - val_loss: -21279725623.3514\n",
      "Epoch 292/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -21469801844.4624 - val_loss: -21464366355.1288\n",
      "Epoch 293/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -22013610485.6840 - val_loss: -21654801351.0207\n",
      "Epoch 294/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -22065903487.8643 - val_loss: -21843496803.7138\n",
      "Epoch 295/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -22255205457.4422 - val_loss: -22033035039.3386\n",
      "Epoch 296/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -22586276183.1432 - val_loss: -22225727940.5787\n",
      "Epoch 297/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -22771435031.3468 - val_loss: -22419191661.4817\n",
      "Epoch 298/600\n",
      "1886/1886 [==============================] - 0s 207us/sample - loss: -22659773536.6448 - val_loss: -22611007657.3100\n",
      "Epoch 299/600\n",
      "1886/1886 [==============================] - 0s 165us/sample - loss: -22774158091.6734 - val_loss: -22803472911.4658\n",
      "Epoch 300/600\n",
      "1886/1886 [==============================] - 0s 204us/sample - loss: -23252499428.8526 - val_loss: -22998243591.7329\n",
      "Epoch 301/600\n",
      "1886/1886 [==============================] - 0s 153us/sample - loss: -23436885051.7243 - val_loss: -23193759789.5835\n",
      "Epoch 302/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -23575976739.5631 - val_loss: -23392822449.4499\n",
      "Epoch 303/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -23583427115.9788 - val_loss: -23588638215.3259\n",
      "Epoch 304/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -24161398228.0212 - val_loss: -23790256714.0731\n",
      "Epoch 305/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -24381866311.9406 - val_loss: -23992108968.0890\n",
      "Epoch 306/600\n",
      "1886/1886 [==============================] - 0s 143us/sample - loss: -24304160214.1930 - val_loss: -24193275617.4754\n",
      "Epoch 307/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -24435236503.4825 - val_loss: -24394731038.1176\n",
      "Epoch 308/600\n",
      "1886/1886 [==============================] - 0s 123us/sample - loss: -24777490288.6617 - val_loss: -24598540099.1542\n",
      "Epoch 309/600\n",
      "1886/1886 [==============================] - 0s 133us/sample - loss: -25335393982.5748 - val_loss: -24806014754.5946\n",
      "Epoch 310/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -25445983302.5832 - val_loss: -25014597465.9459\n",
      "Epoch 311/600\n",
      "1886/1886 [==============================] - 0s 156us/sample - loss: -25254683838.0318 - val_loss: -25219598342.5119\n",
      "Epoch 312/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -25608644151.9236 - val_loss: -25427359307.7011\n",
      "Epoch 313/600\n",
      "1886/1886 [==============================] - 0s 124us/sample - loss: -25965704551.4316 - val_loss: -25638574259.0779\n",
      "Epoch 314/600\n",
      "1886/1886 [==============================] - 0s 187us/sample - loss: -26184835191.4486 - val_loss: -25848802129.8060\n",
      "Epoch 315/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -26397931663.3383 - val_loss: -26062392764.4388\n",
      "Epoch 316/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -26428124838.6851 - val_loss: -26274288670.9316\n",
      "Epoch 317/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -26797339712.0679 - val_loss: -26489185359.7711\n",
      "Epoch 318/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -27255687754.3839 - val_loss: -26705047643.1669\n",
      "Epoch 319/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -27624958312.5175 - val_loss: -26924768280.4197\n",
      "Epoch 320/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -27163184072.6193 - val_loss: -27141577622.1812\n",
      "Epoch 321/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -27770654958.8971 - val_loss: -27359817705.2083\n",
      "Epoch 322/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -28701512218.6045 - val_loss: -27583654610.8235\n",
      "Epoch 323/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -27954348090.6384 - val_loss: -27805787759.5167\n",
      "Epoch 324/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -27799195902.0997 - val_loss: -28022512889.0811\n",
      "Epoch 325/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -28651281958.5493 - val_loss: -28246258256.5851\n",
      "Epoch 326/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -28677164426.1803 - val_loss: -28468588850.0604\n",
      "Epoch 327/600\n",
      "1886/1886 [==============================] - 0s 164us/sample - loss: -29432512844.2842 - val_loss: -28696463675.8283\n",
      "Epoch 328/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -29400520949.4125 - val_loss: -28924910593.6280\n",
      "Epoch 329/600\n",
      "1886/1886 [==============================] - 0s 187us/sample - loss: -29520894473.2301 - val_loss: -29151773759.4913\n",
      "Epoch 330/600\n",
      "1886/1886 [==============================] - 0s 97us/sample - loss: -29580566640.9332 - val_loss: -29380334847.5930\n",
      "Epoch 331/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -29721769100.0806 - val_loss: -29608469007.4658\n",
      "Epoch 332/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -29774387740.7762 - val_loss: -29836215466.9380\n",
      "Epoch 333/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -30204788036.6829 - val_loss: -30064149023.7456\n",
      "Epoch 334/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -30132033538.1718 - val_loss: -30295732556.1081\n",
      "Epoch 335/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -31426295477.8876 - val_loss: -30533775581.4054\n",
      "Epoch 336/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -31294525448.6872 - val_loss: -30771739877.5453\n",
      "Epoch 337/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -31127292744.4836 - val_loss: -31007383477.1129\n",
      "Epoch 338/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -31367416029.5228 - val_loss: -31243583064.7250\n",
      "Epoch 339/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -31121336654.4560 - val_loss: -31477958411.8029\n",
      "Epoch 340/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -31760581203.0710 - val_loss: -31715696070.2067\n",
      "Epoch 341/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -31970226345.3998 - val_loss: -31955385192.5978\n",
      "Epoch 342/600\n",
      "1886/1886 [==============================] - 0s 199us/sample - loss: -32526098358.1591 - val_loss: -32198435030.8935\n",
      "Epoch 343/600\n",
      "1886/1886 [==============================] - 0s 130us/sample - loss: -33561974032.5599 - val_loss: -32447201827.0016\n",
      "Epoch 344/600\n",
      "1886/1886 [==============================] - 0s 157us/sample - loss: -32695951883.4019 - val_loss: -32689794713.8442\n",
      "Epoch 345/600\n",
      "1886/1886 [==============================] - 0s 124us/sample - loss: -33355585520.7975 - val_loss: -32936933600.6614\n",
      "Epoch 346/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -33587391458.6808 - val_loss: -33185467999.2369\n",
      "Epoch 347/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -34204013128.2121 - val_loss: -33436736728.5215\n",
      "Epoch 348/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -33645720044.9968 - val_loss: -33683771756.6677\n",
      "Epoch 349/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -34697765572.0042 - val_loss: -33936643610.8617\n",
      "Epoch 350/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -34769653043.3086 - val_loss: -34189574404.4769\n",
      "Epoch 351/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -34479274974.3372 - val_loss: -34442077957.2909\n",
      "Epoch 352/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -34770840060.1994 - val_loss: -34694950400.8140\n",
      "Epoch 353/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -35231190466.6469 - val_loss: -34947829157.6471\n",
      "Epoch 354/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -35122940822.6681 - val_loss: -35202869772.2099\n",
      "Epoch 355/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -36097361937.3743 - val_loss: -35462419734.3847\n",
      "Epoch 356/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -34987166089.0944 - val_loss: -35716309389.2273\n",
      "Epoch 357/600\n",
      "1886/1886 [==============================] - 0s 149us/sample - loss: -35834688465.3065 - val_loss: -35972842774.3847\n",
      "Epoch 358/600\n",
      "1886/1886 [==============================] - 0s 183us/sample - loss: -36968425999.7455 - val_loss: -36235407613.9650\n",
      "Epoch 359/600\n",
      "1886/1886 [==============================] - 0s 128us/sample - loss: -36493463373.9130 - val_loss: -36496897549.8378\n",
      "Epoch 360/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -37912793183.5589 - val_loss: -36767198151.0207\n",
      "Epoch 361/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -37254433156.7508 - val_loss: -37031834316.3116\n",
      "Epoch 362/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -37896412827.8261 - val_loss: -37299864971.5994\n",
      "Epoch 363/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -37979933497.2810 - val_loss: -37569478167.6057\n",
      "Epoch 364/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -37852167892.2927 - val_loss: -37836387052.8712\n",
      "Epoch 365/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -38308994943.8643 - val_loss: -38104624455.2242\n",
      "Epoch 366/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -39457044269.3362 - val_loss: -38379497410.1367\n",
      "Epoch 367/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -39441668219.7922 - val_loss: -38655962348.0572\n",
      "Epoch 368/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -39435042448.9671 - val_loss: -38930856062.9825\n",
      "Epoch 369/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -39683409496.5005 - val_loss: -39205423061.6725\n",
      "Epoch 370/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -39851072571.7243 - val_loss: -39482327248.3816\n",
      "Epoch 371/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -39699649497.9936 - val_loss: -39757507409.8060\n",
      "Epoch 372/600\n",
      "1886/1886 [==============================] - 0s 176us/sample - loss: -40519704371.8515 - val_loss: -40035152360.3943\n",
      "Epoch 373/600\n",
      "1886/1886 [==============================] - 0s 188us/sample - loss: -41531610512.6957 - val_loss: -40320259851.8029\n",
      "Epoch 374/600\n",
      "1886/1886 [==============================] - 0s 234us/sample - loss: -41541006297.9936 - val_loss: -40606702867.1288\n",
      "Epoch 375/600\n",
      "1886/1886 [==============================] - 0s 134us/sample - loss: -40773316130.2057 - val_loss: -40885359614.3720\n",
      "Epoch 376/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -41118285016.0933 - val_loss: -41165985528.2671\n",
      "Epoch 377/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -41897775164.8102 - val_loss: -41449549290.0223\n",
      "Epoch 378/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -42267726566.7529 - val_loss: -41736753965.9905\n",
      "Epoch 379/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -42625920446.3033 - val_loss: -42024456348.2862\n",
      "Epoch 380/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1886/1886 [==============================] - 0s 109us/sample - loss: -42633124754.3245 - val_loss: -42313975440.0763\n",
      "Epoch 381/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -43481785201.7476 - val_loss: -42605283611.2687\n",
      "Epoch 382/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -43053587341.9809 - val_loss: -42895467511.8601\n",
      "Epoch 383/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -43513167828.5642 - val_loss: -43188317954.0350\n",
      "Epoch 384/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -44776306829.1665 - val_loss: -43485750433.1701\n",
      "Epoch 385/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -44412371304.5175 - val_loss: -43782978522.5564\n",
      "Epoch 386/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -44554480708.4115 - val_loss: -44078761121.1701\n",
      "Epoch 387/600\n",
      "1886/1886 [==============================] - 0s 180us/sample - loss: -45171353788.9459 - val_loss: -44378275651.1542\n",
      "Epoch 388/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -45330508316.7762 - val_loss: -44678980397.9905\n",
      "Epoch 389/600\n",
      "1886/1886 [==============================] - 0s 161us/sample - loss: -45317088448.2036 - val_loss: -44978452509.3037\n",
      "Epoch 390/600\n",
      "1886/1886 [==============================] - 0s 149us/sample - loss: -45363616210.9353 - val_loss: -45277543959.6057\n",
      "Epoch 391/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -45269193038.4560 - val_loss: -45574795309.5835\n",
      "Epoch 392/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -47078971641.7561 - val_loss: -45882970098.9762\n",
      "Epoch 393/600\n",
      "1886/1886 [==============================] - 0s 117us/sample - loss: -46875590625.5949 - val_loss: -46190747153.0938\n",
      "Epoch 394/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -46479714301.8282 - val_loss: -46494083446.4356\n",
      "Epoch 395/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -46887225389.6076 - val_loss: -46799383976.9030\n",
      "Epoch 396/600\n",
      "1886/1886 [==============================] - 0s 133us/sample - loss: -46902044135.5673 - val_loss: -47105061345.8824\n",
      "Epoch 397/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -47846806763.6395 - val_loss: -47416026038.7409\n",
      "Epoch 398/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -47504003371.7073 - val_loss: -47723494636.0572\n",
      "Epoch 399/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -48678057768.9926 - val_loss: -48037053177.8951\n",
      "Epoch 400/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -46912222652.1315 - val_loss: -48343554689.4245\n",
      "Epoch 401/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -49335721467.1135 - val_loss: -48659010890.4801\n",
      "Epoch 402/600\n",
      "1886/1886 [==============================] - 0s 171us/sample - loss: -49995375612.7423 - val_loss: -48981859928.7250\n",
      "Epoch 403/600\n",
      "1886/1886 [==============================] - 0s 142us/sample - loss: -48941192219.1474 - val_loss: -49294547956.6041\n",
      "Epoch 404/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -49998328029.5228 - val_loss: -49616079730.3657\n",
      "Epoch 405/600\n",
      "1886/1886 [==============================] - 0s 157us/sample - loss: -50849854512.8653 - val_loss: -49939171826.1622\n",
      "Epoch 406/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -51203438059.9109 - val_loss: -50265239747.3577\n",
      "Epoch 407/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -51356109987.9703 - val_loss: -50593314324.3498\n",
      "Epoch 408/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -52435236388.3775 - val_loss: -50925384090.2512\n",
      "Epoch 409/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -52265042732.2503 - val_loss: -51255414223.9746\n",
      "Epoch 410/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -51212153891.8346 - val_loss: -51578582131.5866\n",
      "Epoch 411/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -52415207691.1304 - val_loss: -51906675593.1574\n",
      "Epoch 412/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -53161078210.6469 - val_loss: -52241266450.3148\n",
      "Epoch 413/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -52496426771.2747 - val_loss: -52569051871.8474\n",
      "Epoch 414/600\n",
      "1886/1886 [==============================] - 0s 146us/sample - loss: -53509157893.4295 - val_loss: -52904200071.5294\n",
      "Epoch 415/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -53935607525.6670 - val_loss: -53239524334.0922\n",
      "Epoch 416/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -54920483367.6352 - val_loss: -53581030183.4785\n",
      "Epoch 417/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -54318283019.1304 - val_loss: -53917586873.1828\n",
      "Epoch 418/600\n",
      "1886/1886 [==============================] - 0s 192us/sample - loss: -53979406353.3743 - val_loss: -54252020623.6693\n",
      "Epoch 419/600\n",
      "1886/1886 [==============================] - 0s 135us/sample - loss: -55312131798.4645 - val_loss: -54592753559.8092\n",
      "Epoch 420/600\n",
      "1886/1886 [==============================] - 0s 181us/sample - loss: -54982504339.4104 - val_loss: -54934283776.8140\n",
      "Epoch 421/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -55866090114.8505 - val_loss: -55278392637.4563\n",
      "Epoch 422/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -56465387085.6416 - val_loss: -55624175472.7377\n",
      "Epoch 423/600\n",
      "1886/1886 [==============================] - 0s 134us/sample - loss: -56828600133.2259 - val_loss: -55971441496.3180\n",
      "Epoch 424/600\n",
      "1886/1886 [==============================] - 0s 119us/sample - loss: -56799857810.5960 - val_loss: -56317825408.2035\n",
      "Epoch 425/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -57911417494.3966 - val_loss: -56670075716.7822\n",
      "Epoch 426/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -57367722789.7349 - val_loss: -57019225509.6471\n",
      "Epoch 427/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -56496592993.7307 - val_loss: -57362088821.6216\n",
      "Epoch 428/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -58229535771.1474 - val_loss: -57715722282.3275\n",
      "Epoch 429/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -58845445731.3595 - val_loss: -58069625870.6518\n",
      "Epoch 430/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -58500062215.6013 - val_loss: -58423894706.2639\n",
      "Epoch 431/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -59350280055.1771 - val_loss: -58781353056.0509\n",
      "Epoch 432/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -59931573685.6161 - val_loss: -59139456086.2830\n",
      "Epoch 433/600\n",
      "1886/1886 [==============================] - 0s 146us/sample - loss: -61173513084.6066 - val_loss: -59505234304.2035\n",
      "Epoch 434/600\n",
      "1886/1886 [==============================] - 0s 154us/sample - loss: -60969436278.3627 - val_loss: -59872271400.6995\n",
      "Epoch 435/600\n",
      "1886/1886 [==============================] - 0s 193us/sample - loss: -62532451848.1442 - val_loss: -60242548485.2909\n",
      "Epoch 436/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -61138633355.5376 - val_loss: -60608043662.4483\n",
      "Epoch 437/600\n",
      "1886/1886 [==============================] - 0s 130us/sample - loss: -61413614593.0859 - val_loss: -60970904577.6280\n",
      "Epoch 438/600\n",
      "1886/1886 [==============================] - 0s 129us/sample - loss: -61863990881.1877 - val_loss: -61339148906.6327\n",
      "Epoch 439/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -61895050258.4602 - val_loss: -61702856664.9285\n",
      "Epoch 440/600\n",
      "1886/1886 [==============================] - 0s 121us/sample - loss: -62638661501.6925 - val_loss: -62070211523.7647\n",
      "Epoch 441/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -62248042482.9692 - val_loss: -62439342060.4642\n",
      "Epoch 442/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -63751039095.4486 - val_loss: -62811444873.5644\n",
      "Epoch 443/600\n",
      "1886/1886 [==============================] - 0s 119us/sample - loss: -64732084286.9820 - val_loss: -63189871044.5787\n",
      "Epoch 444/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -65693300568.7720 - val_loss: -63572387446.0286\n",
      "Epoch 445/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -65271498927.9152 - val_loss: -63952683574.5374\n",
      "Epoch 446/600\n",
      "1886/1886 [==============================] - 0s 179us/sample - loss: -64308885580.0127 - val_loss: -64327161969.9587\n",
      "Epoch 447/600\n",
      "1886/1886 [==============================] - 0s 139us/sample - loss: -65783332030.0318 - val_loss: -64709332705.4754\n",
      "Epoch 448/600\n",
      "1886/1886 [==============================] - 0s 199us/sample - loss: -65235325579.5376 - val_loss: -65088808644.1717\n",
      "Epoch 449/600\n",
      "1886/1886 [==============================] - 0s 146us/sample - loss: -66848575149.2004 - val_loss: -65470439254.6900\n",
      "Epoch 450/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -66719911992.4666 - val_loss: -65854191905.7806\n",
      "Epoch 451/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -66646126344.4157 - val_loss: -66236593964.3625\n",
      "Epoch 452/600\n",
      "1886/1886 [==============================] - 0s 124us/sample - loss: -68098652692.0891 - val_loss: -66630089273.7933\n",
      "Epoch 453/600\n",
      "1886/1886 [==============================] - 0s 146us/sample - loss: -67438654148.0042 - val_loss: -67016447645.1002\n",
      "Epoch 454/600\n",
      "1886/1886 [==============================] - 0s 136us/sample - loss: -68818959736.8059 - val_loss: -67409475658.8871\n",
      "Epoch 455/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -67728870833.2725 - val_loss: -67795653591.3005\n",
      "Epoch 456/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -70080920170.9608 - val_loss: -68193353688.9285\n",
      "Epoch 457/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -68854657523.5122 - val_loss: -68584840603.8792\n",
      "Epoch 458/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -70210452487.6013 - val_loss: -68981043968.4070\n",
      "Epoch 459/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -71020223491.2577 - val_loss: -69383296805.8506\n",
      "Epoch 460/600\n",
      "1886/1886 [==============================] - 0s 185us/sample - loss: -70303570317.4380 - val_loss: -69778956984.7758\n",
      "Epoch 461/600\n",
      "1886/1886 [==============================] - 0s 133us/sample - loss: -70701037522.3924 - val_loss: -70178807234.9507\n",
      "Epoch 462/600\n",
      "1886/1886 [==============================] - 0s 194us/sample - loss: -70073740421.5652 - val_loss: -70573888139.1924\n",
      "Epoch 463/600\n",
      "1886/1886 [==============================] - 0s 129us/sample - loss: -72480783298.1039 - val_loss: -70978700527.3132\n",
      "Epoch 464/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -72024584066.0361 - val_loss: -71380642687.3895\n",
      "Epoch 465/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -72377468356.8187 - val_loss: -71785383841.5771\n",
      "Epoch 466/600\n",
      "1886/1886 [==============================] - 0s 119us/sample - loss: -73783584101.2598 - val_loss: -72197932440.6232\n",
      "Epoch 467/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -74982510961.2047 - val_loss: -72613559343.2114\n",
      "Epoch 468/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -74008524884.6999 - val_loss: -73025820870.6137\n",
      "Epoch 469/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -74627814744.2291 - val_loss: -73438666478.4992\n",
      "Epoch 470/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -75243129928.7550 - val_loss: -73853542960.0254\n",
      "Epoch 471/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -75699082089.0604 - val_loss: -74268586133.7742\n",
      "Epoch 472/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -76397291287.6182 - val_loss: -74691101987.4086\n",
      "Epoch 473/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -75623558147.2577 - val_loss: -75105689313.4754\n",
      "Epoch 474/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -75371356073.1283 - val_loss: -75517926759.7838\n",
      "Epoch 475/600\n",
      "1886/1886 [==============================] - 0s 180us/sample - loss: -77780687484.3351 - val_loss: -75945066652.2862\n",
      "Epoch 476/600\n",
      "1886/1886 [==============================] - 0s 136us/sample - loss: -76984553502.4051 - val_loss: -76365396804.7822\n",
      "Epoch 477/600\n",
      "1886/1886 [==============================] - 0s 157us/sample - loss: -78198844271.5758 - val_loss: -76790513377.4754\n",
      "Epoch 478/600\n",
      "1886/1886 [==============================] - 0s 131us/sample - loss: -77745919528.7211 - val_loss: -77213477785.4372\n",
      "Epoch 479/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -78082060535.5843 - val_loss: -77637839157.3164\n",
      "Epoch 480/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -78217982321.2047 - val_loss: -78062677618.7727\n",
      "Epoch 481/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -79384941616.8653 - val_loss: -78492189752.9793\n",
      "Epoch 482/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -78845861080.0933 - val_loss: -78919798543.0588\n",
      "Epoch 483/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -80355282157.8112 - val_loss: -79352135432.5469\n",
      "Epoch 484/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -80186442892.0806 - val_loss: -79786111034.6073\n",
      "Epoch 485/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -82183751178.3160 - val_loss: -80224061202.3148\n",
      "Epoch 486/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -82021095239.3977 - val_loss: -80664459983.5676\n",
      "Epoch 487/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -82198410627.6649 - val_loss: -81104730050.1367\n",
      "Epoch 488/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -82730862880.8483 - val_loss: -81547056061.2528\n",
      "Epoch 489/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -83975633180.5048 - val_loss: -81995491324.7440\n",
      "Epoch 490/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -83438770590.8123 - val_loss: -82439072535.1987\n",
      "Epoch 491/600\n",
      "1886/1886 [==============================] - 0s 198us/sample - loss: -84744788956.1654 - val_loss: -82887734674.1113\n",
      "Epoch 492/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -85936088056.3987 - val_loss: -83340695737.5898\n",
      "Epoch 493/600\n",
      "1886/1886 [==============================] - 0s 175us/sample - loss: -83630634806.0233 - val_loss: -83785996747.0906\n",
      "Epoch 494/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -85372955729.4422 - val_loss: -84234432414.3212\n",
      "Epoch 495/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -86467847942.2439 - val_loss: -84688923740.7949\n",
      "Epoch 496/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -87470944182.1591 - val_loss: -85149240360.6995\n",
      "Epoch 497/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -85610896000.6787 - val_loss: -85598502052.4261\n",
      "Epoch 498/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -85964780558.1167 - val_loss: -86048572954.8617\n",
      "Epoch 499/600\n",
      "1886/1886 [==============================] - 0s 115us/sample - loss: -85848938519.8897 - val_loss: -86499230229.9777\n",
      "Epoch 500/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -88795569229.0986 - val_loss: -86959765116.5405\n",
      "Epoch 501/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -89364215614.7105 - val_loss: -87424864275.5358\n",
      "Epoch 502/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -89905327315.7497 - val_loss: -87893929117.9142\n",
      "Epoch 503/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -90502314867.9194 - val_loss: -88360478000.4324\n",
      "Epoch 504/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1886/1886 [==============================] - 0s 116us/sample - loss: -89089438021.7688 - val_loss: -88821995953.0429\n",
      "Epoch 505/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -90209199209.3319 - val_loss: -89289148829.5072\n",
      "Epoch 506/600\n",
      "1886/1886 [==============================] - 0s 247us/sample - loss: -90628534693.3277 - val_loss: -89756290883.1542\n",
      "Epoch 507/600\n",
      "1886/1886 [==============================] - 0s 204us/sample - loss: -90459838496.5769 - val_loss: -90223734933.7742\n",
      "Epoch 508/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -92020146071.7540 - val_loss: -90696212414.8808\n",
      "Epoch 509/600\n",
      "1886/1886 [==============================] - 0s 97us/sample - loss: -92072594895.6776 - val_loss: -91169078410.3784\n",
      "Epoch 510/600\n",
      "1886/1886 [==============================] - 0s 94us/sample - loss: -94013356863.7964 - val_loss: -91648295654.3593\n",
      "Epoch 511/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -92511129543.5334 - val_loss: -92123990286.2448\n",
      "Epoch 512/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -95971050540.5217 - val_loss: -92610889946.1494\n",
      "Epoch 513/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -94499500105.8409 - val_loss: -93093177015.1479\n",
      "Epoch 514/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -94694463225.2132 - val_loss: -93573863987.2814\n",
      "Epoch 515/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -95389852455.9067 - val_loss: -94056805739.0397\n",
      "Epoch 516/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -95285900823.3468 - val_loss: -94539576422.5628\n",
      "Epoch 517/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -95519536784.9671 - val_loss: -95024731017.1574\n",
      "Epoch 518/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -96988665011.1729 - val_loss: -95513003064.9793\n",
      "Epoch 519/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -97686994110.0318 - val_loss: -96003299956.4006\n",
      "Epoch 520/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -98363907852.7593 - val_loss: -96497366339.9682\n",
      "Epoch 521/600\n",
      "1886/1886 [==============================] - 0s 198us/sample - loss: -99000681368.8399 - val_loss: -96996387151.3641\n",
      "Epoch 522/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -99306374249.3319 - val_loss: -97491521842.0604\n",
      "Epoch 523/600\n",
      "1886/1886 [==============================] - 0s 188us/sample - loss: -99336665442.0021 - val_loss: -97988051829.6216\n",
      "Epoch 524/600\n",
      "1886/1886 [==============================] - 0s 96us/sample - loss: -100393755953.1368 - val_loss: -98488548060.5914\n",
      "Epoch 525/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -99379211319.3807 - val_loss: -98984998211.9682\n",
      "Epoch 526/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -100782169608.1442 - val_loss: -99485523186.5692\n",
      "Epoch 527/600\n",
      "1886/1886 [==============================] - 0s 111us/sample - loss: -100441744185.2810 - val_loss: -99984667711.4913\n",
      "Epoch 528/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -100128962140.8441 - val_loss: -100483397075.2305\n",
      "Epoch 529/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -101839949854.4051 - val_loss: -100984631432.7504\n",
      "Epoch 530/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -102275953452.2503 - val_loss: -101494331447.3513\n",
      "Epoch 531/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -103614761379.1559 - val_loss: -102001157828.1717\n",
      "Epoch 532/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -103004463729.4761 - val_loss: -102509552657.9078\n",
      "Epoch 533/600\n",
      "1886/1886 [==============================] - 0s 109us/sample - loss: -106595813855.9661 - val_loss: -103032475459.1542\n",
      "Epoch 534/600\n",
      "1886/1886 [==============================] - 0s 110us/sample - loss: -104344093549.4040 - val_loss: -103544623670.5374\n",
      "Epoch 535/600\n",
      "1886/1886 [==============================] - 0s 102us/sample - loss: -104707038050.5451 - val_loss: -104060529991.2242\n",
      "Epoch 536/600\n",
      "1886/1886 [==============================] - 0s 127us/sample - loss: -105327834629.9724 - val_loss: -104576929915.7265\n",
      "Epoch 537/600\n",
      "1886/1886 [==============================] - 0s 157us/sample - loss: -107417816753.5440 - val_loss: -105098352864.6614\n",
      "Epoch 538/600\n",
      "1886/1886 [==============================] - 0s 107us/sample - loss: -107431426766.8632 - val_loss: -105623165413.1383\n",
      "Epoch 539/600\n",
      "1886/1886 [==============================] - 0s 186us/sample - loss: -108455476865.7646 - val_loss: -106150337843.6884\n",
      "Epoch 540/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -106135345862.1760 - val_loss: -106667277476.4261\n",
      "Epoch 541/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -108515527127.2789 - val_loss: -107193681799.5294\n",
      "Epoch 542/600\n",
      "1886/1886 [==============================] - 0s 129us/sample - loss: -107985854429.2513 - val_loss: -107718902697.7170\n",
      "Epoch 543/600\n",
      "1886/1886 [==============================] - 0s 150us/sample - loss: -110814307737.3828 - val_loss: -108251622453.7234\n",
      "Epoch 544/600\n",
      "1886/1886 [==============================] - 0s 154us/sample - loss: -110800141380.4115 - val_loss: -108786509234.6709\n",
      "Epoch 545/600\n",
      "1886/1886 [==============================] - 0s 113us/sample - loss: -109966131721.2301 - val_loss: -109319321603.2560\n",
      "Epoch 546/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -111146282551.9237 - val_loss: -109854633821.2019\n",
      "Epoch 547/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -112358690107.9958 - val_loss: -110392610002.0095\n",
      "Epoch 548/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -112656415847.1601 - val_loss: -110933134077.1510\n",
      "Epoch 549/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -112567003225.0435 - val_loss: -111474656482.2894\n",
      "Epoch 550/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -115093835789.0308 - val_loss: -112019621663.3386\n",
      "Epoch 551/600\n",
      "1886/1886 [==============================] - 0s 167us/sample - loss: -114635772933.4295 - val_loss: -112566171534.0413\n",
      "Epoch 552/600\n",
      "1886/1886 [==============================] - 0s 137us/sample - loss: -112864125770.6553 - val_loss: -113103557461.0620\n",
      "Epoch 553/600\n",
      "1886/1886 [==============================] - 0s 159us/sample - loss: -116667430961.9512 - val_loss: -113655222278.5119\n",
      "Epoch 554/600\n",
      "1886/1886 [==============================] - 0s 122us/sample - loss: -117556121439.2874 - val_loss: -114214732935.1224\n",
      "Epoch 555/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -114744125068.6236 - val_loss: -114758269074.5183\n",
      "Epoch 556/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -115495918338.9862 - val_loss: -115309024553.9205\n",
      "Epoch 557/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -116904366805.3786 - val_loss: -115858195486.9316\n",
      "Epoch 558/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -116990590437.3956 - val_loss: -116405404208.0254\n",
      "Epoch 559/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -116852184382.1676 - val_loss: -116957844491.3959\n",
      "Epoch 560/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -119342044973.3362 - val_loss: -117520904003.1542\n",
      "Epoch 561/600\n",
      "1886/1886 [==============================] - 0s 99us/sample - loss: -119820446297.5864 - val_loss: -118079940474.5056\n",
      "Epoch 562/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -118172826553.4167 - val_loss: -118635530158.6009\n",
      "Epoch 563/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -119690547586.5790 - val_loss: -119195303506.2130\n",
      "Epoch 564/600\n",
      "1886/1886 [==============================] - 0s 95us/sample - loss: -119908362806.8378 - val_loss: -119757510620.1844\n",
      "Epoch 565/600\n",
      "1886/1886 [==============================] - 0s 96us/sample - loss: -123928496998.8887 - val_loss: -120331451491.3068\n",
      "Epoch 566/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -121521537345.4252 - val_loss: -120900288811.5485\n",
      "Epoch 567/600\n",
      "1886/1886 [==============================] - 0s 126us/sample - loss: -122374971053.2004 - val_loss: -121470236232.4451\n",
      "Epoch 568/600\n",
      "1886/1886 [==============================] - 0s 168us/sample - loss: -121846933297.6797 - val_loss: -122037397630.9825\n",
      "Epoch 569/600\n",
      "1886/1886 [==============================] - 0s 121us/sample - loss: -123126636092.2672 - val_loss: -122607115675.8792\n",
      "Epoch 570/600\n",
      "1886/1886 [==============================] - 0s 188us/sample - loss: -127234591886.2524 - val_loss: -123194989966.8553\n",
      "Epoch 571/600\n",
      "1886/1886 [==============================] - 0s 145us/sample - loss: -126175384737.7985 - val_loss: -123778021053.6598\n",
      "Epoch 572/600\n",
      "1886/1886 [==============================] - 0s 108us/sample - loss: -126047976160.2375 - val_loss: -124363396328.8013\n",
      "Epoch 573/600\n",
      "1886/1886 [==============================] - 0s 131us/sample - loss: -126357878616.7720 - val_loss: -124940690280.5978\n",
      "Epoch 574/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -125810457564.1654 - val_loss: -125520943263.5421\n",
      "Epoch 575/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -126777049374.6766 - val_loss: -126104621661.6089\n",
      "Epoch 576/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -128552918596.9544 - val_loss: -126689035542.3847\n",
      "Epoch 577/600\n",
      "1886/1886 [==============================] - 0s 105us/sample - loss: -126593334305.6628 - val_loss: -127271490818.8490\n",
      "Epoch 578/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -130523723200.4751 - val_loss: -127864787328.2035\n",
      "Epoch 579/600\n",
      "1886/1886 [==============================] - 0s 101us/sample - loss: -127471951837.2513 - val_loss: -128451756175.2623\n",
      "Epoch 580/600\n",
      "1886/1886 [==============================] - 0s 88us/sample - loss: -128409373334.3966 - val_loss: -129036461790.2194\n",
      "Epoch 581/600\n",
      "1886/1886 [==============================] - 0s 98us/sample - loss: -131565435624.9247 - val_loss: -129632561956.2226\n",
      "Epoch 582/600\n",
      "1886/1886 [==============================] - 0s 103us/sample - loss: -131953654965.3447 - val_loss: -130236695400.5978\n",
      "Epoch 583/600\n",
      "1886/1886 [==============================] - 0s 217us/sample - loss: -133237611466.7911 - val_loss: -130836182522.3021\n",
      "Epoch 584/600\n",
      "1886/1886 [==============================] - 0s 121us/sample - loss: -136031723398.3796 - val_loss: -131454165942.7409\n",
      "Epoch 585/600\n",
      "1886/1886 [==============================] - 1s 300us/sample - loss: -132724039419.3849 - val_loss: -132052667784.3434\n",
      "Epoch 586/600\n",
      "1886/1886 [==============================] - 0s 148us/sample - loss: -134414138123.6734 - val_loss: -132658313440.6614\n",
      "Epoch 587/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -135325488550.4136 - val_loss: -133269515174.4610\n",
      "Epoch 588/600\n",
      "1886/1886 [==============================] - 0s 125us/sample - loss: -136070894927.5419 - val_loss: -133878135125.8760\n",
      "Epoch 589/600\n",
      "1886/1886 [==============================] - 0s 120us/sample - loss: -138148960731.6225 - val_loss: -134494006638.2957\n",
      "Epoch 590/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -136040761172.4284 - val_loss: -135105790277.5962\n",
      "Epoch 591/600\n",
      "1886/1886 [==============================] - 0s 114us/sample - loss: -137613062261.2768 - val_loss: -135721376046.8045\n",
      "Epoch 592/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -136437851719.1262 - val_loss: -136330076537.6916\n",
      "Epoch 593/600\n",
      "1886/1886 [==============================] - 0s 112us/sample - loss: -138523060478.0997 - val_loss: -136946429854.3212\n",
      "Epoch 594/600\n",
      "1886/1886 [==============================] - 0s 104us/sample - loss: -136635415837.5907 - val_loss: -137554961504.0509\n",
      "Epoch 595/600\n",
      "1886/1886 [==============================] - 0s 100us/sample - loss: -141031500603.4528 - val_loss: -138180391843.2051\n",
      "Epoch 596/600\n",
      "1886/1886 [==============================] - 0s 116us/sample - loss: -142226867270.5833 - val_loss: -138810146933.2146\n",
      "Epoch 597/600\n",
      "1886/1886 [==============================] - 0s 156us/sample - loss: -141287481352.6872 - val_loss: -139435603932.1844\n",
      "Epoch 598/600\n",
      "1886/1886 [==============================] - 0s 118us/sample - loss: -144372764572.0976 - val_loss: -140073564892.5914\n",
      "Epoch 599/600\n",
      "1886/1886 [==============================] - 0s 204us/sample - loss: -143605456053.3446 - val_loss: -140708345090.8490\n",
      "Epoch 600/600\n",
      "1886/1886 [==============================] - 0s 106us/sample - loss: -140265658548.2587 - val_loss: -141325359615.1860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cf8a518908>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop,board]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\fit\n"
     ]
    }
   ],
   "source": [
    "print(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mateusz\\\\Desktop\\\\WSBProjekt'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Użyj cd w wierszu poleceń, aby zmienić katalog na ścieżkę pliku zgłoszoną przez pwd lub bieżącą lokalizację pliku .py\n",
    "### Następnie uruchom ten kod w wierszu polecenia lub terminalu, wpisując - 'tensorboard --logdir logs\\fit'\n",
    "### Tensorboard będzie działał lokalnie w twojej przeglądarce pod adresem [http://localhost:6006/](http://localhost:6006/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs\\fit (started 0:24:49 ago; pid 19220)\n"
     ]
    }
   ],
   "source": [
    "# Możemy użyć interfejsów API tensorboard.notebook, aby uzyskać nieco większą kontrolę:\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
